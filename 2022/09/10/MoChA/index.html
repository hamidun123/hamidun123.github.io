<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hamidun123.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-1}},"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="介绍单调注意力机制MoChA">
<meta property="og:type" content="article">
<meta property="og:title" content="MoChA">
<meta property="og:url" content="https://hamidun123.github.io/2022/09/10/MoChA/index.html">
<meta property="og:site_name" content="宇坤の博客">
<meta property="og:description" content="介绍单调注意力机制MoChA">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210916151749700.png">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210917103001959.png">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210917103024193.png">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210918092836079.png">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210918095038044.png">
<meta property="og:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210918110215791.png">
<meta property="article:published_time" content="2022-09-10T12:58:03.000Z">
<meta property="article:modified_time" content="2023-01-12T13:05:05.788Z">
<meta property="article:author" content="Yukun Qian">
<meta property="article:tag" content="单调注意力机制">
<meta property="article:tag" content="MoChA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hamidun123.github.io/2022/09/10/MoChA/img/image-20210916151749700.png">


<link rel="canonical" href="https://hamidun123.github.io/2022/09/10/MoChA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://hamidun123.github.io/2022/09/10/MoChA/","path":"2022/09/10/MoChA/","title":"MoChA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MoChA | 宇坤の博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">宇坤の博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">1.</span> <span class="nav-text">1.标准注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AC%E5%8D%95%E8%B0%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.</span> <span class="nav-text">2.硬单调注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E8%B0%83%E5%9D%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6monotonic-chunkwise-attention-mocha"><span class="nav-number">3.</span> <span class="nav-text">3.单调块注意力机制（Monotonic
Chunkwise Attention， MoChA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E8%B0%83%E8%87%AA%E9%80%82%E5%BA%94%E5%88%86%E5%9D%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-monotonic-adaptive-chunkwise-attention-matcha"><span class="nav-number">4.</span> <span class="nav-text">4.单调自适应分块注意力机制
(Monotonic Adaptive Chunkwise Attention, MAtChA)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yukun Qian"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Yukun Qian</p>
  <div class="site-description" itemprop="description">科研记录</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hamidun123" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hamidun123" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qianyukun123@foxmail.com" title="E-Mail → mailto:qianyukun123@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hamidun123.github.io/2022/09/10/MoChA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Yukun Qian">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="宇坤の博客">
      <meta itemprop="description" content="科研记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MoChA | 宇坤の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MoChA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-10 20:58:03" itemprop="dateCreated datePublished" datetime="2022-09-10T20:58:03+08:00">2022-09-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-12 21:05:05" itemprop="dateModified" datetime="2023-01-12T21:05:05+08:00">2023-01-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>介绍单调注意力机制MoChA</p>
<span id="more"></span>
<p><img src="img/image-20210916151749700.png" /></p>
<p>ICLR 2018 的一篇论文</p>
<p>作者认为传统的 Soft-Attention 存在着计算复杂度和输入输出序列长度T,U
乘积正相关，复杂度为O（TU），因此不适合长序列计算，且在计算attention过程中由于需要等待输入完成才可以开始计算，因此无法做到实时解码。Raffel
等在2017年指出当输入序列和输出序列直接不需要重排的时候可以简化运算如图(b)，但是其使用的Hard
monotonic attention太强硬限制了性能，因此提出了改进版本Monotonic
chunkwise attention (MoChA)。</p>
<p>对于序列到序列模型，假设输入是<span
class="math inline">\(x=\{x_1,\dots,x_T\}\)</span>，输出是<span
class="math inline">\(y=\{y_1,\dots,y_U\}\)</span>，一般来说输入会被编码器转换为隐藏序列<span
class="math inline">\(h=\{h_1,\dots,h_T\}\)</span> <span
class="math display">\[
h_j=EncoderRNN(x_j,h_{j-1}) \tag{1}
\]</span> 然后解码器自己迭代计算隐藏向量和输出，这其中<span
class="math inline">\(s_i\)</span>是解码器隐藏状态，<span
class="math inline">\(c_i\)</span>是上下文向量，<span
class="math inline">\(y_i\)</span>是输出结果，需要注意的是<span
class="math inline">\(c_i\)</span>是解码器和编码器沟通的唯一通道 <span
class="math display">\[
s_i=DecoderRNN(y_{i-1},s_{i-1},c_i) \tag{2}
\]</span></p>
<p><span class="math display">\[
y_i=Output(s_i,c_i) \tag{3}
\]</span></p>
<h3 id="标准注意力机制">1.标准注意力机制</h3>
<figure>
<img src="img/image-20210917103001959.png"
alt="image-20210917103001959" />
<figcaption aria-hidden="true">image-20210917103001959</figcaption>
</figure>
<p>首先在解码器步骤，根据<span
class="math inline">\(s_{i-1}\)</span>为每个编码器隐藏状态<span
class="math inline">\(h_j\)</span>计算能量值<span
class="math inline">\(e_{i,j}\)</span> <span class="math display">\[
e_{i,j}=Energy(h_j,s_{i-1}) \tag{4}
\]</span> 这里的<span
class="math inline">\(Energy(\cdot)\)</span>有很多选择，一种常见的是，其中<span
class="math inline">\(W_h,W_s,b,v\)</span>是可以学习的参数 <span
class="math display">\[
Energy(h_j,s_{i-1}):=v^Ttanh(W_hh_j+W_ss_{i-1}+b) \tag{5}
\]</span> 之后对<span class="math inline">\(e_{i,j}\)</span>使用<span
class="math inline">\(softmax\)</span>便可以得到权重值 <span
class="math display">\[
\alpha_{i,j}=\frac{exp(e_{i,j})}{\sum_{k=1}^{T}exp(e_{i,k})}=softmax(e_{i,:})j
\tag{5}
\]</span> 最后利用<span
class="math inline">\(\alpha_{i,j}\)</span>对<span
class="math inline">\(h_j\)</span>进行加权，便可以得到上下文向量<span
class="math inline">\(c_i\)</span> <span class="math display">\[
c_i=\sum_{j=1}^{T}\alpha_{i,j}h_j \tag{6}
\]</span></p>
<h3 id="硬单调注意力机制">2.硬单调注意力机制</h3>
<figure>
<img src="img/image-20210917103024193.png"
alt="image-20210917103024193" />
<figcaption aria-hidden="true">image-20210917103024193</figcaption>
</figure>
<p>硬单调注意力机制，在每个解码时间步<span
class="math inline">\(i\)</span>，注意力机制会从上一个时间步所关注的注意力索引，记为<span
class="math inline">\(t_{i-1}\)</span>，开始向后搜索，对于<span
class="math inline">\(j=t_{i-1},
t_{i-1}+1,\dots\)</span>计算每一步的<span
class="math inline">\(e_{i,j}\)</span>，随后利用<span
class="math inline">\(\sigma(\cdot)\)</span>将其转换为选择概率<span
class="math inline">\(p_{i,j}\)</span>，经过随机伯努利采样，得到最后的离散结果<span
class="math inline">\(z_{i,j}\)</span> <span class="math display">\[
\begin{gather}
e_{i,j} = MonotonicEnergy(s_{i-1},h_j) \tag{7}
\\p_{i,j} = \sigma(e_{i,j}) \tag{8}
\\z_{i,j} \backsim Bernoulli(p_{i,j}) \tag{9}
\end{gather}
\]</span></p>
<p>一旦出现<span
class="math inline">\(z_{i,j}\)</span>等于1，模型将会停止，并设置<span
class="math inline">\(t_i=j，c_i=h_t\)</span>，其复杂度是<span
class="math inline">\(O(max(T,U))\)</span>，同时还可以被用于实时解码</p>
<p>但是由于上述过程涉及到了采样，其无法利用反向传播，因此在训练过程需要改为对<span
class="math inline">\(c_i\)</span>的期望值进行训练，其被改为如下形式
<span class="math display">\[
\begin{gather}
\alpha_{i,j}=p_{i,j}
\left((1-p_{i,j-1})\frac{\alpha_{i,j-1}}{p_{i,j-1}}+\alpha_{i-1,j}
\right) \tag{10}
\\
c_i=\sum_{j=1}^{T}\alpha_{i,j}h_j \tag{11}
\end{gather}
\]</span> 这里主要对第一个公式进行解释，我们定义<span
class="math inline">\(\alpha_{1,j}\)</span>表示第<span
class="math inline">\(j\)</span>个被选中而第<span
class="math inline">\(1,2,\dots,j-1\)</span>没被选中的概率，则 <span
class="math display">\[
\alpha_{1,j}=p_{1,j}\prod_{k=1}^{j-1}(1-p_{1,k}) \tag{12}
\]</span> 当<span class="math inline">\(i&gt;0\)</span>时，对于<span
class="math inline">\(\alpha_{i,j}\)</span>来说，为了在第<span
class="math inline">\(i\)</span>个时间步选中第<span
class="math inline">\(j\)</span>个编码器步骤，除了满足<span
class="math inline">\(c_i=h_j\)</span>以外，还需要满足<span
class="math inline">\(c_{i-1}=h_{k},
k\in\{1,\dots,j\}\)</span>（单调注意力机制）则 <span
class="math display">\[
\alpha_{i,j}=p_{i,j}\sum_{k=1}^{j}\left(\alpha_{i-1,k}
\prod_{l=k}^{j-1}(1-p_{i,l}) \right) \tag{13}
\]</span> 这里<span
class="math inline">\(\alpha_{i-1,k}\)</span>描述的是第<span
class="math inline">\(i-1\)</span>个时间步选择<span
class="math inline">\(k\)</span>的概率，其乘以在第<span
class="math inline">\(i\)</span>个时间步不选择<span
class="math inline">\((k,k+1,\dots,j-1)\)</span>的概率<span
class="math inline">\(\prod_{l=k}^{j-1}(1-p_{i,l}\)</span>)</p>
<p>这里我们为了方便，定义<span
class="math inline">\(\prod_{n}^mx=1,n&gt;m\)</span>，这里式（12）也可以用（13）来表示，只不过需要定义
<span class="math display">\[
\alpha_{0,j}=\begin{cases}
0, \,\,j=1
\\1, \,\,j\in\{2,\dots,T\} \tag{14}
\end{cases}
\]</span> 接下来将从公式（13）推到出公式（10）的形式</p>
<p>由公式（13），先分离出<span
class="math inline">\(a_{i-i,j}\)</span>，再从连乘中分离出<span
class="math inline">\((1-p_{i,j-1})\)</span>，就得到了公式（10）的形式
<span class="math display">\[
\begin{align}
\alpha_{i,j} &amp;= p_{i,j} \left( \sum_{k=1}^{j-1} \left(
\alpha_{i-1,k} \prod_{l=k}^{j-1}(1-p_{i,l})\right) + \alpha_{i-1,j}
\right)
\\ &amp;= p_{i,j}\left( (1-p_{i,j-1}) \sum_{k=1}^{j-1}
\left(\alpha_{i-1,k} \prod_{l=k}^{j-2}(1-p_{i,l})\right) +
\alpha_{i-1,j} \right)
\\ &amp;= p_{i,j}
\left((1-p_{i,j-1})\frac{\alpha_{i,j-1}}{p_{i,j-1}}+\alpha_{i-1,j}
\right)
\end{align}
\]</span> 对于公式（10）也可以这么理解，<span
class="math inline">\(\alpha_{i,j-1}\)</span>表示第i步选择<span
class="math inline">\(j-1\)</span>项的概率，但是为了修正实际没选的事实，需要乘以<span
class="math inline">\((1-p_{i,j-1})/p_{i,j-1}\)</span>进行修正，再加上<span
class="math inline">\(\alpha_{i-1,j}\)</span>表征上一步选择<span
class="math inline">\(j\)</span>的概率，最后乘以<span
class="math inline">\(p_{i,j}\)</span>强制选择<span
class="math inline">\(j\)</span></p>
<p>对于公式（10）来说<span
class="math inline">\(\alpha_{i,j}\)</span>的计算依赖于<span
class="math inline">\(\alpha_{i-1,j}\)</span>，<span
class="math inline">\(\alpha_{i,j-1}\)</span>，而对于<span
class="math inline">\(\alpha_{i,j-1}\)</span>的依赖则意味着需要计算<span
class="math inline">\(\alpha_{i,1},\alpha_{i,2},\dots,\alpha_{i,T}\)</span>，然而可以通过一些方法将其计算化简</p>
<p>定义<span
class="math inline">\(q_{i,j}=\alpha_{i,j}/p_{i,j}\)</span>，则由公式（10）可得
<span class="math display">\[
\begin{align}
q_{i,j} &amp;= (1-p_{i,j-1})q_{i,j-1}+\alpha_{i-1,j} \tag{15}
\cr  q_{i,j}-(1-p_{i,j-1})q_{i,j-1} &amp;= \alpha_{i-1,j} \tag{16}
\cr  \frac{q_{i,j}}{\prod_{k=1}^{j}(1-p_{i,k-1})} -
\frac{(1-p_{i,j-1})q_{i,j-1}}{\prod_{k=1}^{j}(1-p_{i,k-1})}
&amp;=  \frac{ \alpha_{i-1,j}}{\prod_{k=1}^{j}(1-p_{i,k-1})} \tag{17}
\cr  \frac{q_{i,j}}{\prod_{k=1}^{j}(1-p_{i,k-1})} -
\frac{q_{i,j-1}}{\prod_{k=1}^{j-1}(1-p_{i,k-1})} &amp;=  \frac{
\alpha_{i-1,j}}{\prod_{k=1}^{j}(1-p_{i,k-1})} \tag{18}
\cr  \sum_{l=1}^{j}\left( \frac{q_{i,l}}{\prod_{k=1}^{l}(1-p_{i,k-1})} -
\frac{q_{i,l-1}}{\prod_{k=1}^{l-1}(1-p_{i,k-1})}\right) &amp;=
\sum_{l=1}^{j} \frac{ \alpha_{i-1,l}}{\prod_{k=1}^{l}(1-p_{i,k-1})}
\tag{19}
\cr  \frac{q_{i,j}}{\prod_{k=1}^{j}(1-p_{i,k-1})}-q_{i,0} &amp;=
\sum_{l=1}^{j} \frac{ \alpha_{i-1,l}}{\prod_{k=1}^{l}(1-p_{i,k-1})}
\tag{20}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
q_{i,j} &amp;= \left(\prod_{k=1}^{j}(1-p_{i,k-1})\right)  \left(
\sum_{l=1}^{j} \frac{ \alpha_{i-1,l}}{\prod_{k=1}^{l}(1-p_{i,k-1})}
\right) \tag{21}
\\  q_i &amp;= cumprod(1-p_i)cumsum(\frac{\alpha_{i-1}}{cumprod(1-p_i)})
\tag{22}
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(cumprod(x)\)</span>和<span
class="math inline">\(cumsum(x)\)</span>的定义如下：</p>
<p><span class="math display">\[
cumprod(x) = [1,x_1,x_1x_2,\dots,\prod_{i}^{|x|-1}x_i]
\]</span></p>
<p><span class="math display">\[
cumsum(x) = [x_1,x_1+x_2,\dots,\sum_{i}^{|x|}x_i]
\]</span></p>
<p>同时作者指出，在一些任务上使用原来的<span
class="math inline">\(Energy(\cdot)\)</span>函数可能面临无法收敛，此时可以改为
<span class="math display">\[
MonotonicEnergy(s_{i-1},h_j) =
g\frac{v^T}{||v||}tanh(W_ss_{i-1}+W_hh_j)+r \tag{23}
\]</span></p>
<h3
id="单调块注意力机制monotonic-chunkwise-attention-mocha">3.单调块注意力机制（Monotonic
Chunkwise Attention， MoChA）</h3>
<figure>
<img src="img/image-20210918092836079.png"
alt="image-20210918092836079" />
<figcaption aria-hidden="true">image-20210918092836079</figcaption>
</figure>
<p>上一节中的硬单调注意力机制，有两个重要的限制：1.每次只注意一个编码器结果
2.输入输出对齐式严格单调的。作者认为由于这两个的限制导致了硬单调注意力机制在大部分任务上不如传统注意力机制，因此提出了修改模型MoChA</p>
<p>在test阶段，其和硬单调注意力机制基本一致，只是在计算<span
class="math inline">\(c_i\)</span>的时候，改为了计算包括<span
class="math inline">\(t_i\)</span>在内共<span
class="math inline">\(w\)</span>长度的普通注意力机制，即在小范围内实现软注意力机制
<span class="math display">\[
\begin{align}
v &amp;= t_i-w+1  \tag{24}
\\ u_{i,k} &amp;=
ChunkEnergy(s_{i-1,h_k}),k\in\{v,v+1,\dots,t_i\}  \tag{25}
\\ c_i &amp;=
\sum_{k=v}^{t_i}\frac{exp(u_{i,k})}{\sum_{l=v}^{t_i}exp(u_i,l)}h_k  \tag{26}
\end{align}
\]</span>
其中公式（25）的能量函数与公式（5）类似，和公式（23）的不同</p>
<p>这里作者给出了在test阶段的伪代码</p>
<figure>
<img src="img/image-20210918095038044.png"
alt="image-20210918095038044" />
<figcaption aria-hidden="true">image-20210918095038044</figcaption>
</figure>
<p>在训练阶段仿照第二部分的方法，计算<span
class="math inline">\(c_i\)</span>的期望值分布，这里记为<span
class="math inline">\(\beta_{i,j}\)</span>，其中外层的求和考虑了包含<span
class="math inline">\(j\)</span>位置的所有可能情况的<span
class="math inline">\(\alpha\)</span> <span class="math display">\[
\beta_{i,j}=\sum_{k=j}^{j+w-1}\left(\frac{\alpha_{i,k}exp(u_{i,j})}{\sum_{l=k-w+1}^{k}exp(u_{i,l})}\right)
\tag{27}
\]</span> 在这里引入<span
class="math inline">\(MovingSum(\cdot)\)</span> <span
class="math display">\[
MovingSum(x,b,f)_n:=\sum_{m=n-(b-1)}^{n+f-1}x_m
\]</span> 则式（27）可以改写为： <span class="math display">\[
\beta_{i,:}=exp(u_{i,:})MovingSum\left(\frac{\alpha_{i,:}}{MovingSum(exp(u_{i,:}),w,1)},1,w\right)
\tag{28}
\]</span> 最后给出在训练期间MoChA的全部公式 <span
class="math display">\[
\begin{align}
e_{i,j} &amp;= MonotonicEnergy(s_{i-1},h_j) \tag{29}
\\ \epsilon &amp;\backsim \mathcal{N}(0,1) \tag{30}
\\ p_{i,j} &amp;= \sigma(e_{i,j}+\epsilon) \tag{31}
\\ \alpha_{i,:} &amp;=
p_{i,:}cumprod(1-p_{i,:})cumsum(\frac{\alpha_{i-1,:}}{cumprod(1-p_{i,:})})
\tag{32}
\\ u_{i,j} &amp;= ChunkEnergy(s_{i-1,h_j}) \tag{33}
\\ \beta_{i,:} &amp;=
exp(u_{i,:})MovingSum\left(\frac{\alpha_{i,:}}{MovingSum(exp(u_{i,:}),w,1)},1,w\right)
\tag{34}
\\ c_i &amp;=\sum_{j=1}^{T}\beta_{i,j}h_j \tag{35}
\end{align}
\]</span></p>
<h3
id="单调自适应分块注意力机制-monotonic-adaptive-chunkwise-attention-matcha">4.单调自适应分块注意力机制
(Monotonic Adaptive Chunkwise Attention, MAtChA)</h3>
<figure>
<img src="img/image-20210918110215791.png"
alt="image-20210918110215791" />
<figcaption aria-hidden="true">image-20210918110215791</figcaption>
</figure>
<p>作者在附录给出了自适应分块的推导，理论上其可以自适应的选择块的大小，但按照作者所描述的，该方法并没有表现的更好</p>
<p>在test阶段，其和MoChA十分相似，但是在计算<span
class="math inline">\(c_i\)</span>的时候，其中的<span
class="math inline">\(v\)</span>被替换为<span
class="math inline">\(t_{i-1}\)</span>即计算一般注意力的范围从上一时间步关注的点到当前步为止
<span class="math display">\[
c_i=\sum_{k=t_{i-1}}^{t_i}\frac{exp(u_{i,k})}{\sum_{l=t_{i-1}}^{t_i}exp(u_i,l)}h_k
\tag{36}
\]</span> 在训练阶段则开始有点复杂了</p>
<p>首先给出<span class="math inline">\(\beta_{i,j}\)</span>的表达式
<span class="math display">\[
\beta_{i,j} =
\sum_{k=1}^{j}\sum_{l=j}^{T}\left(\frac{exp(u_{i,j})}{\sum_{m=k}^{l}exp(u_{i,m})}\alpha_{i-1,k}p_{i,l}\prod_{n=k}^{l-1}(1-p_{i,n})\right)
\tag{37}
\]</span> 这个表达式是这样理解的，首先对于第<span
class="math inline">\(i\)</span>时间步的第<span
class="math inline">\(j\)</span>个位置来说，其上一时间步的位置对当前位置是有影响的，因此需要将所有可能性相加，即<span
class="math inline">\(\sum_{k=1}^{j}\)</span>，而对于每一个<span
class="math inline">\(k\)</span>当前时间步的位置从<span
class="math inline">\(j\)</span>开始到<span
class="math inline">\(T\)</span>也是都有可能，因此也需要相加即<span
class="math inline">\(\sum_{l=j}^{T}\)</span>，在括号内第一部分<span
class="math inline">\(\frac{exp(u_{i,j})}{\sum_{m=k}^{l}exp(u_{i,m})}\)</span>则是对<span
class="math inline">\(k\)</span>到<span
class="math inline">\(l\)</span>区间的软注意力计算，之后乘以上一时间步选择<span
class="math inline">\(k\)</span>位置的概率<span
class="math inline">\(\alpha_{i-1,k}\)</span>，接下来再乘以连续不选<span
class="math inline">\(k,k+1,\dots,l-1\)</span>的概率和选择<span
class="math inline">\(l\)</span>的概率<span
class="math inline">\(p_{i,l}\prod_{n=k}^{l-1}(1-p_{i,n})\)</span></p>
<p>公式（37）需要进行嵌套计算，在这里给出一个可以并行计算的方法 <span
class="math display">\[
\begin{align}
\beta_{i,j} &amp;=
\sum_{k=1}^{j}\sum_{l=j}^{T}\left(\frac{exp(u_{i,j})}{\sum_{m=k}^{l}exp(u_{i,m})}\alpha_{i-1,k}p_{i,l}\prod_{n=k}^{l-1}(1-p_{i,n})\right)
\tag{38}
\\ &amp;=
exp(u_{i,j})\sum_{k=1}^{j}\sum_{l=j}^{T}\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}p_{i,l}\prod_{n=k}^{l-1}(1-p_{i,n})\right)
\tag{39}
\\ &amp;=
exp(u_{i,j})\sum_{l=j}^{T}\sum_{k=1}^{j}\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}p_{i,l}\prod_{n=k}^{l-1}(1-p_{i,n})\right)
\tag{40}
\\ &amp;=
exp(u_{i,j})\sum_{l=j}^{T}p_{i,l}\sum_{k=1}^{j}\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{l-1}(1-p_{i,n})\right)
\tag{41}
\\ &amp;=
exp(u_{i,j})\sum_{l=j}^{T}p_{i,l}\sum_{k=1}^{j}\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{j-1}(1-p_{i,n})
\prod_{o=j}^{l-1}(1-p_{i,o}) \right) \tag{42}
\\ &amp;= exp(u_{i,j})\sum_{l=j}^{T} p_{i,l}
\prod_{o=j}^{l-1}(1-p_{i,o}) \sum_{k=1}^{j}
\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{j-1}(1-p_{i,n})
\right) \tag{43}
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
r_{i,j,l} &amp;= \sum_{k=1}^{j}
\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{j-1}(1-p_{i,n})
\right) \tag{44}
\\ &amp;= \sum_{k=1}^{j-1}
\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{j-1}(1-p_{i,n})
\right) + \frac{\alpha_{i-1,j}}{\sum_{m=j}^{l}exp(u_{i,m})} \tag{45}
\\ &amp;= (1-p_{i,j-1})\sum_{k=1}^{j-1}
\left(\frac{\alpha_{i-1,k}}{\sum_{m=k}^{l}exp(u_{i,m})}\prod_{n=k}^{j-2}(1-p_{i,n})
\right) + \frac{\alpha_{i-1,j}}{\sum_{m=j}^{l}exp(u_{i,m})} \tag{46}
\\ &amp;= (1-p_{i,j-1})r_{i,j-1,l} +
\frac{\alpha_{i-1,j}}{\sum_{m=j}^{l}exp(u_{i,m})} \tag{47}
\end{align}
\]</span> 通过式（44）将（43）重写为 <span class="math display">\[
\beta_{i,j} = exp(u_{i,j})\sum_{l=j}^{T} p_{i,l}
\prod_{o=j}^{l-1}(1-p_{i,o}) r_{i,j,l} \tag{48}
\]</span> 经过观察式（47）与式（15）很像，根据式（15-22）可以推导出
<span class="math display">\[
r_{i,:,:} = cumprod(1-p_{i,:})cumsum\left(
\frac{\alpha_{i-1,:}}{AllPartialSums(exp(u_{i,:})cumprod(1-p_{i,:}))}
\right) \tag{49}
\]</span> 其中 <span class="math display">\[
AllPartialSums(x)_{j,l}=  \begin{cases}
\sum_{m=j}^{l} x_m, \, \, j \leq l
\\ 1, \, \, j&gt;l
\end{cases}
\]</span></p>
<p>则 <span class="math display">\[
\beta_{i,:}=exp(u_i,:)\sum_{t=j}^{T}p_{i,:}AllPartialSums(1-p_{i,:})_{:,l}
\cdot r_{i,:,l}
\]</span>
根据作者所描述，该方法目前相比MoChA没有提升，计算复杂度却大大上升</p>
<p>参考文献</p>
<p>[1] CHIU C-C, RAFFEL C. Monotonic chunkwise attention [J]. arXiv
preprint arXiv:171205382, 2017.</p>
<p>[2] RAFFEL C, LUONG M-T, LIU P J, et al. Online and linear-time
attention by enforcing monotonic alignments; proceedings of the
International Conference on Machine Learning, F, 2017 [C]. PMLR.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="Yukun Qian 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Yukun Qian 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Yukun Qian
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://hamidun123.github.io/2022/09/10/MoChA/" title="MoChA">https://hamidun123.github.io/2022/09/10/MoChA/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%8D%95%E8%B0%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="tag"># 单调注意力机制</a>
              <a href="/tags/MoChA/" rel="tag"># MoChA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/10/Streaming%20ASR/" rel="prev" title="Streaming ASR">
                  <i class="fa fa-chevron-left"></i> Streaming ASR
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/10/ASR%E6%96%87%E7%8C%AE/" rel="next" title="ASR文献">
                  ASR文献 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yukun Qian</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/hamidun123" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"hamidun123","repo":"hamidun123.github.io","client_id":"843ef81eb1a845e082a7","client_secret":"3fcace7a38908d390ad486440c7e6e3f9bf19a6a","admin_user":"hamidun123","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"089eab6a5a40f6146a3db626cc954958"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
